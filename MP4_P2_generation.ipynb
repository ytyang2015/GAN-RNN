{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generating Text with an RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "import unidecode\n",
    "import string\n",
    "import random\n",
    "import re\n",
    "import time\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from rnn.model import RNN\n",
    "from rnn.helpers import time_since\n",
    "from rnn.generate import generate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\") "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Processing\n",
    "\n",
    "The file we are using is a plain text file. We turn any potential unicode characters into plain ASCII by using the `unidecode` package (which you can install via `pip` or `conda`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "file_len = 4573338\n",
      "train len:  4116004\n",
      "test len:  457334\n"
     ]
    }
   ],
   "source": [
    "all_characters = string.printable\n",
    "n_characters = len(all_characters)\n",
    "\n",
    "file_path = './shakespeare.txt'\n",
    "file = unidecode.unidecode(open(file_path).read())\n",
    "file_len = len(file)\n",
    "print('file_len =', file_len)\n",
    "\n",
    "# we will leave the last 1/10th of text as test\n",
    "split = int(0.9*file_len)\n",
    "train_text = file[:split]\n",
    "test_text = file[split:]\n",
    "\n",
    "print('train len: ', len(train_text))\n",
    "print('test len: ', len(test_text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "to Aufidius thus\n",
      "I will appear, and fight.\n",
      "\n",
      "LARTIUS:\n",
      "Now the fair goddess, Fortune,\n",
      "Fall deep in love with thee; and her great charms\n",
      "Misguide thy opposers' swords! Bold gentleman,\n",
      "Prosperity be thy pa\n"
     ]
    }
   ],
   "source": [
    "chunk_len = 200\n",
    "\n",
    "def random_chunk(text):\n",
    "    start_index = random.randint(0, len(text) - chunk_len)\n",
    "    end_index = start_index + chunk_len + 1\n",
    "    return text[start_index:end_index]\n",
    "\n",
    "print(random_chunk(train_text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Input and Target data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To make training samples out of the large string of text data, we will be splitting the text into chunks.\n",
    "\n",
    "Each chunk will be turned into a tensor, specifically a `LongTensor` (used for integer values), by looping through the characters of the string and looking up the index of each character in `all_characters`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Turn string into list of longs\n",
    "def char_tensor(string):\n",
    "    tensor = torch.zeros(len(string), requires_grad=True).long()\n",
    "    for c in range(len(string)):\n",
    "        tensor[c] = all_characters.index(string[c])\n",
    "    return tensor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following function loads a batch of input and target tensors for training. Each sample comes from a random chunk of text. A sample input will consist of all characters *except the last*, while the target wil contain all characters *following the first*. For example: if random_chunk='abc', then input='ab' and target='bc'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_random_batch(text, chunk_len, batch_size):\n",
    "    input_data = torch.zeros(batch_size, chunk_len).long().to(device)\n",
    "    target = torch.zeros(batch_size, chunk_len).long().to(device)\n",
    "    for i in range(batch_size):\n",
    "        start_index = random.randint(0, len(text) - chunk_len - 1)\n",
    "        end_index = start_index + chunk_len + 1\n",
    "        chunk = text[start_index:end_index]\n",
    "        input_data[i] = char_tensor(chunk[:-1])\n",
    "        target[i] = char_tensor(chunk[1:])\n",
    "    return input_data, target"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implement model\n",
    "\n",
    "Your RNN model will take as input the character for step $t_{-1}$ and output a prediction for the next character $t$. The model should consiste of three layers - a linear layer that encodes the input character into an embedded state, an RNN layer (which may itself have multiple layers) that operates on that embedded state and a hidden state, and a decoder layer that outputs the predicted character scores distribution.\n",
    "\n",
    "\n",
    "You must implement your model in the `rnn/model.py` file. You should use a `nn.Embedding` object for the encoding layer, a RNN model like `nn.RNN` or `nn.LSTM`, and a `nn.Linear` layer for the final a predicted character score decoding layer.\n",
    "\n",
    "\n",
    "**TODO:** Implement the model in RNN `rnn/model.py`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluating\n",
    "\n",
    "To evaluate the network we will feed one character at a time, use the outputs of the network as a probability distribution for the next character, and repeat. To start generation we pass a priming string to start building up the hidden state, from which we then generate one character at a time.\n",
    "\n",
    "\n",
    "Note that in the `evaluate` function, every time a prediction is made the outputs are divided by the \"temperature\" argument. Higher temperature values make actions more equally likely giving more \"random\" outputs. Lower temperature values (less than 1) high likelihood options contribute more. A temperature near 0 outputs only the most likely outputs.\n",
    "\n",
    "You may check different temperature values yourself, but we have provided a default which should work well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(rnn, prime_str='A', predict_len=100, temperature=0.8):\n",
    "    hidden = rnn.init_hidden(1, device=device)\n",
    "    prime_input = char_tensor(prime_str)\n",
    "    predicted = prime_str\n",
    "\n",
    "    # Use priming string to \"build up\" hidden state\n",
    "    for p in range(len(prime_str) - 1):\n",
    "        _, hidden = rnn(prime_input[p].unsqueeze(0).to(device), hidden)\n",
    "    inp = prime_input[-1]\n",
    "    \n",
    "    for p in range(predict_len):\n",
    "        output, hidden = rnn(inp.unsqueeze(0).to(device), hidden)\n",
    "        \n",
    "        # Sample from the network as a multinomial distribution\n",
    "        output_dist = output.data.view(-1).div(temperature).exp()\n",
    "        top_i = torch.multinomial(output_dist, 1)[0]\n",
    "        \n",
    "        # Add predicted character to string and use as next input\n",
    "        predicted_char = all_characters[top_i]\n",
    "        predicted += predicted_char\n",
    "        inp = char_tensor(predicted_char)\n",
    "\n",
    "    return predicted"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 100\n",
    "n_epochs = 5000\n",
    "hidden_size = 100\n",
    "n_layers = 1\n",
    "learning_rate = 0.01\n",
    "model_type = 'rnn'\n",
    "print_every = 50\n",
    "plot_every = 50\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_test(rnn, inp, target):\n",
    "    with torch.no_grad():\n",
    "        hidden = rnn.init_hidden(batch_size, device=device)\n",
    "        loss = 0\n",
    "        for c in range(chunk_len):\n",
    "            output, hidden = rnn(inp[:,c], hidden)\n",
    "            loss += criterion(output.view(batch_size, -1), target[:,c])\n",
    "    \n",
    "    return loss.data.item() / chunk_len"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train function\n",
    "\n",
    "**TODO**: Fill in the train function. You should initialize a hidden layer representation using your RNN's `init_hidden` function, set the model gradients to zero, and loop over each time step (character) in the input tensor. For each time step compute the output of the of the RNN and compute the loss over the output and the corresponding ground truth time step in `target`. The loss should be averaged over all time steps. Lastly, call backward on the averaged loss and take an optimizer step.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train1(rnn, input, target, optimizer, criterion):\n",
    "    \"\"\"\n",
    "    Inputs:\n",
    "    - rnn: model\n",
    "    - input: input character data tensor of shape (batch_size, chunk_len)\n",
    "    - target: target character data tensor of shape (batch_size, chunk_len)\n",
    "    - optimizer: rnn model optimizer\n",
    "    - criterion: loss function\n",
    "    \n",
    "    Returns:\n",
    "    - loss: computed loss value as python float\n",
    "    \"\"\"\n",
    "    loss = None\n",
    "    \n",
    "    ####################################\n",
    "    #          YOUR CODE HERE          #\n",
    "    ####################################\n",
    "    rnn.zero_grad()\n",
    "    batch_size, length = input.shape\n",
    "#     print(input.shape)\n",
    "    \n",
    "    hidden_start = rnn.init_hidden(batch_size, device)\n",
    "    \n",
    "    hidden = hidden_start.detach()\n",
    "#     print(hidden.size())#torch.Size([1, 100, 100])\n",
    "    \n",
    "    \n",
    "    running_loss = torch.zeros((1)).to(device)\n",
    "    optimizer.zero_grad()\n",
    "    for i in range(length):\n",
    "        \n",
    "        curr_input = input[:,i] #torch.Size([100, 200])\n",
    "#         print(curr_input.size())#torch.Size([100])\n",
    "        curr_target = target[:,i]\n",
    "        output, hidden = rnn(curr_input, hidden) ######error\n",
    "        \n",
    "        running_loss += criterion(output.view(batch_size, -1), curr_target.view(batch_size))\n",
    "        \n",
    "    running_loss /= length\n",
    "    running_loss.backward()\n",
    "    optimizer.step()\n",
    "    loss = running_loss.data.cpu().numpy()[0]   \n",
    "    \n",
    "    ##########       END      ##########\n",
    "\n",
    "    return loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(rnn, input, target, optimizer, criterion):\n",
    "    \"\"\"\n",
    "    Inputs:\n",
    "    - rnn: model\n",
    "    - input: input character data tensor of shape (batch_size, chunk_len)\n",
    "    - target: target character data tensor of shape (batch_size, chunk_len)\n",
    "    - optimizer: rnn model optimizer\n",
    "    - criterion: loss function\n",
    "    \n",
    "    Returns:\n",
    "    - loss: computed loss value as python float\n",
    "    \"\"\"\n",
    "    ####################################\n",
    "    #          YOUR CODE HERE          #\n",
    "    ####################################\n",
    "    \n",
    "    batch_size, length = input.shape\n",
    "    rnn.zero_grad()\n",
    "    hidden = rnn.init_hidden(batch_size,device)\n",
    "    loss = 0\n",
    "\n",
    "    for i in range(length):\n",
    "        output, hidden = rnn(input[:, i],hidden)\n",
    "        loss+=criterion(output, target[:, i])\n",
    "    loss /= length\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    ##########       END      ##########\n",
    "    \n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class RNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size, model_type=\"rnn\", n_layers=1):\n",
    "        super(RNN, self).__init__()\n",
    "        \"\"\"\n",
    "        Initialize the RNN model.\n",
    "\n",
    "        You should create:\n",
    "        - An Embedding object which will learn a mapping from tensors\n",
    "        of dimension input_size to embedding of dimension hidden_size.\n",
    "        - Your RNN network which takes the embedding as input (use models\n",
    "        in torch.nn). This network should have input size hidden_size and\n",
    "        output size hidden_size.\n",
    "        - A linear layer of dimension hidden_size x output_size which\n",
    "        will predict output scores.\n",
    "\n",
    "        Inputs:\n",
    "        - input_size: Dimension of individual element in input sequence to model\n",
    "        - hidden_size: Hidden layer dimension of RNN model\n",
    "        - output_size: Dimension of individual element in output sequence from model\n",
    "        - model_type: RNN network type can be \"rnn\" (for basic rnn), \"gru\", or \"lstm\"\n",
    "        - n_layers: number of layers in your RNN network\n",
    "        \"\"\"\n",
    "\n",
    "        self.model_type = model_type\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.output_size = output_size\n",
    "        self.n_layers = n_layers\n",
    "\n",
    "        ####################################\n",
    "        #          YOUR CODE HERE          #\n",
    "        ####################################\n",
    "        self.encoder = nn.Embedding(self.input_size, self.hidden_size)\n",
    "\n",
    "        if model_type == \"rnn\":\n",
    "            self.rnn = nn.RNN(hidden_size, hidden_size, n_layers)\n",
    "        elif model_type == \"lstm\":\n",
    "            self.rnn = nn.LSTM(hidden_size, hidden_size, n_layers)\n",
    "        elif model_type == \"gru\":\n",
    "            self.rnn = nn.GRU(hidden_size, hidden_size, n_layers)\n",
    "\n",
    "        self.decoder = nn.Linear(self.hidden_size, self.output_size)\n",
    "        ##########       END      ##########\n",
    "\n",
    "\n",
    "\n",
    "    def forward(self, input, hidden):\n",
    "        \"\"\"\n",
    "        Forward pass through RNN model. Use your Embedding object to create\n",
    "        an embedded input to your RNN network. You should then use the\n",
    "        linear layer to get an output of self.output_size.\n",
    "\n",
    "        Inputs:\n",
    "        - input: the input data tensor to your model of dimension (batch_size)\n",
    "        - hidden: the hidden state tensor of dimension (n_layers x batch_size x hidden_size)\n",
    "\n",
    "        Returns:\n",
    "        - output: the output of your linear layer\n",
    "        - hidden: the output of the RNN network before your linear layer (hidden state)\n",
    "        \"\"\"\n",
    "\n",
    "\n",
    "        ####################################\n",
    "        #          YOUR CODE HERE          #\n",
    "        ####################################\n",
    "        batch_size = input.size()[0]\n",
    "        \n",
    "        emb = self.encoder(input).view(1,batch_size,self.hidden_size)\n",
    "        if self.model_type == \"lstm\":\n",
    "            emb = emb.contiguous().view(emb.size(0), emb.size(1), -1)\n",
    "        \n",
    "        output, hidden = self.rnn(emb, hidden) \n",
    "#         decoded = self.decoder(output.view(output.size(0)*output.size(1), output.size(2)))\n",
    "        decoded = self.decoder(output)\n",
    "#         output = decoded.view(output.size(0), output.size(1), decoded.size(1))\n",
    "        output = decoded.view(batch_size, -1)\n",
    "        ##########       END      ##########\n",
    "\n",
    "\n",
    "        return output, hidden\n",
    "\n",
    "    def init_hidden(self, batch_size, device=None):\n",
    "        \"\"\"\n",
    "        Initialize hidden states to all 0s during training.\n",
    "\n",
    "        Hidden states should be initilized to dimension (n_layers x batch_size x hidden_size)\n",
    "\n",
    "        Inputs:\n",
    "        - batch_size: batch size\n",
    "\n",
    "        Returns:\n",
    "        - hidden: initialized hidden values for input to forward function\n",
    "        \"\"\"\n",
    "\n",
    "        hidden = None\n",
    "\n",
    "        ####################################\n",
    "        #          YOUR CODE HERE          #\n",
    "        ####################################\n",
    "\n",
    "        weight = next(self.parameters())\n",
    "        if self.model_type == 'lstm':\n",
    "            hidden = (weight.new_zeros(self.n_layers, batch_size, self.hidden_size),\n",
    "                    weight.new_zeros(self.n_layers, batch_size, self.hidden_size))\n",
    "        else:\n",
    "            hidden = weight.new_zeros(self.n_layers, batch_size, self.hidden_size)\n",
    "\n",
    "\n",
    "        ##########       END      ##########\n",
    "\n",
    "        return hidden\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training for 5000 epochs...\n",
      "[0m 33s (50 1%) train loss: 2.1164, test_loss: 2.1330]\n",
      "Whe reager her havint bear whath thy\n",
      "I Ghee: stee heaver shead conied is withing of tllowe not in gren \n",
      "\n",
      "[1m 8s (100 2%) train loss: 1.9458, test_loss: 1.9723]\n",
      "Whe the difter;\n",
      "Ford\n",
      "be to the gonest with tope me are this?\n",
      "\n",
      "DOUCKINWA:\n",
      "Ol dock somef the wither.\n",
      "\n",
      "SI \n",
      "\n",
      "[1m 42s (150 3%) train loss: 1.8601, test_loss: 1.8829]\n",
      "Whan prom slears\n",
      "The grative I do this and whome thou aboe it the poor on jud sword the sires?\n",
      "And are \n",
      "\n",
      "[2m 16s (200 4%) train loss: 1.8213, test_loss: 1.8375]\n",
      "Who dowers dot the mark:\n",
      "That belible for better, hore,\n",
      "And Is sabe; gotand, by to so man good this tr \n",
      "\n",
      "[2m 50s (250 5%) train loss: 1.7737, test_loss: 1.8416]\n",
      "Whe, me nit.\n",
      "\n",
      "MARINE:\n",
      "Is your his are:\n",
      "Grition, where ifat, whom he, sure of the purse I bear a maw fr \n",
      "\n",
      "[3m 24s (300 6%) train loss: 1.7654, test_loss: 1.8038]\n",
      "Whong,\n",
      "Well:\n",
      "There not, to his of embery fitle of grings compance. But my beath sirneral for the sines \n",
      "\n",
      "[3m 58s (350 7%) train loss: 1.7586, test_loss: 1.7809]\n",
      "Why must a love so bation, it:\n",
      "I have is mo, thee; the brother fation, what my will could happot hears \n",
      "\n",
      "[4m 32s (400 8%) train loss: 1.7145, test_loss: 1.7797]\n",
      "Whom you find curoh, see men are quees to thee Englany the peace?\n",
      "\n",
      "MARK NICHARD EPHELLO:\n",
      "Yer find me t \n",
      "\n",
      "[5m 6s (450 9%) train loss: 1.7425, test_loss: 1.7531]\n",
      "Whom them, so of Lay the man the find; and the this but for the such of our has? which catherightice c \n",
      "\n",
      "[5m 39s (500 10%) train loss: 1.6996, test_loss: 1.7606]\n",
      "Whold in they are masterm they are now with merous and they bestens a heart the mandor, rease the will \n",
      "\n",
      "[6m 13s (550 11%) train loss: 1.6841, test_loss: 1.7864]\n",
      "When his arming against the liest with much comes good the libblore of men venieve.\n",
      "\n",
      "MARCELLO:\n",
      "Why, if \n",
      "\n",
      "[6m 47s (600 12%) train loss: 1.6877, test_loss: 1.7849]\n",
      "Why, good a soond,\n",
      "Child of the men: my poor Tome as it you with the laugh your, he was of this were l \n",
      "\n",
      "[7m 21s (650 13%) train loss: 1.6937, test_loss: 1.7372]\n",
      "Whes a should bur?\n",
      "\n",
      "Fooks by vated you so wook, which seemence to with his know the mishion.\n",
      "\n",
      "GONZALES \n",
      "\n",
      "[7m 55s (700 14%) train loss: 1.7015, test_loss: 1.7437]\n",
      "Whed not quarry more.\n",
      "\n",
      "HENRY CAVELONIUS:\n",
      "Aet did, and out the wors pist deaths of say speak to his abo \n",
      "\n",
      "[8m 29s (750 15%) train loss: 1.7047, test_loss: 1.7274]\n",
      "Where, him of that fird licked once on him not?\n",
      "\n",
      "OTHELLO:\n",
      "A sperch.\n",
      "\n",
      "MARANTIO:\n",
      "They will not it.\n",
      "\n",
      "BIRI \n",
      "\n",
      "[9m 3s (800 16%) train loss: 1.6656, test_loss: 1.7603]\n",
      "When we but a glorest inversent: the lame;\n",
      "What's this it so, he be, he body for ear stable stors\n",
      "Pay  \n",
      "\n",
      "[9m 36s (850 17%) train loss: 1.6765, test_loss: 1.7395]\n",
      "Which for, and the door pace.\n",
      "\n",
      "DROMIO LEAR:\n",
      "Her hot nothing his come and did you fool.\n",
      "\n",
      "Cercome the my \n",
      "\n",
      "[10m 13s (900 18%) train loss: 1.6462, test_loss: 1.7282]\n",
      "When\n",
      "Thou have I beer's were of Well me innoth as the pale.\n",
      "\n",
      "CLORIOLAN:\n",
      "What of me scorn'd for the con \n",
      "\n",
      "[10m 49s (950 19%) train loss: 1.6517, test_loss: 1.7491]\n",
      "Why, and in'timed gods my own of me wrong\n",
      "Disper he myself.\n",
      "\n",
      "JOM:\n",
      "Well in unchest the worth ang a vall \n",
      "\n",
      "[11m 24s (1000 20%) train loss: 1.6563, test_loss: 1.7575]\n",
      "Why, there of the pass my peoper were come your think me, courtor:\n",
      "Now I wift the let pieral were part \n",
      "\n",
      "[11m 58s (1050 21%) train loss: 1.6772, test_loss: 1.7268]\n",
      "When. A'\n",
      "But a prince of my love, give of the grace of him:\n",
      "You seners,\n",
      "Firce:\n",
      "And men so may there to \n",
      "\n",
      "[12m 32s (1100 22%) train loss: 1.6252, test_loss: 1.7412]\n",
      "Why send, my fortune.\n",
      "\n",
      "PRINCE:\n",
      "Ay, I meay the tright about to reverse.\n",
      "\n",
      "MACDUFF:\n",
      "So the marship\n",
      "Harged \n",
      "\n",
      "[13m 6s (1150 23%) train loss: 1.6510, test_loss: 1.6890]\n",
      "What where saving in his countaly to finare to on\n",
      "His when, chite truy, we darce\n",
      "shall well\n",
      "That we'll \n",
      "\n",
      "[13m 42s (1200 24%) train loss: 1.6589, test_loss: 1.7025]\n",
      "When I did chis of the eid\n",
      "Hands! what what better fight the word,\n",
      "Or hath there strike man such more  \n",
      "\n",
      "[14m 18s (1250 25%) train loss: 1.6444, test_loss: 1.7358]\n",
      "Where help meet for your bustion meant me to more of grace its Sight to love him lord graced you are w \n",
      "\n",
      "[14m 53s (1300 26%) train loss: 1.6579, test_loss: 1.7342]\n",
      "While,\n",
      "He some no land will say thy be so too at erement very is find incorst that he ar next gays to  \n",
      "\n",
      "[15m 31s (1350 27%) train loss: 1.6414, test_loss: 1.7200]\n",
      "Whis all dog their will believe the king accusent this time along didstal, they come my lord, go.\n",
      "Good \n",
      "\n",
      "[16m 6s (1400 28%) train loss: 1.6108, test_loss: 1.7397]\n",
      "Why knepden with the good.\n",
      "\n",
      "IAGOF:\n",
      "You should I sate.\n",
      "\n",
      "CAGESS:\n",
      "Since so fortunes whom the bring speak. \n",
      "\n",
      "[16m 40s (1450 28%) train loss: 1.6567, test_loss: 1.7558]\n",
      "When:\n",
      "He Steep a voubt efce, come of Rome, both:\n",
      "Them all this,\n",
      "With life of how a false\n",
      "He thou God t \n",
      "\n",
      "[17m 14s (1500 30%) train loss: 1.6484, test_loss: 1.7216]\n",
      "Why ven at wwim:\n",
      "I curse stant of more to them, I should we revengles the pertary usinest to the lord  \n",
      "\n",
      "[17m 48s (1550 31%) train loss: 1.6320, test_loss: 1.7354]\n",
      "Why,\n",
      "There appented, for the fareis me, my do ares now news noble of mence.\n",
      "\n",
      "KING HENRY:\n",
      "'Tis be here, \n",
      "\n",
      "[18m 23s (1600 32%) train loss: 1.6325, test_loss: 1.7490]\n",
      "Why say.\n",
      "\n",
      "MARCET:\n",
      "This is mercy cropes the conchion is not-days with give dog none, and now about life \n",
      "\n",
      "[18m 58s (1650 33%) train loss: 1.6324, test_loss: 1.7234]\n",
      "Why which me no beserigness than where your ween for meet thou cannot no fool.\n",
      "\n",
      "ANGELO:\n",
      "Dicken of thei \n",
      "\n",
      "[19m 32s (1700 34%) train loss: 1.6195, test_loss: 1.7226]\n",
      "Wh:\n",
      "Let his do to out-war your ord made mind in itiendst off thy land for thee and faul of with thee!\n",
      " \n",
      "\n",
      "[20m 8s (1750 35%) train loss: 1.6469, test_loss: 1.7247]\n",
      "Which do you ruint'st me the scate thy servide the sorrewelly many in unclestand still are in you shir \n",
      "\n",
      "[20m 43s (1800 36%) train loss: 1.6672, test_loss: 1.7415]\n",
      "Whow foonce.\n",
      "\n",
      "LAFEU:\n",
      "The should seem! Let shall be cains, sweace.\n",
      "\n",
      "FALSTAFF:\n",
      "Now berase made of wither \n",
      "\n",
      "[21m 18s (1850 37%) train loss: 1.6438, test_loss: 1.7335]\n",
      "Why monege oft have me satess thy fool. If come,\n",
      "Who, wo lord, which.\n",
      "\n",
      "First Lord, now most fight, mus \n",
      "\n",
      "[21m 52s (1900 38%) train loss: 1.6247, test_loss: 1.7099]\n",
      "Why thou for you honour; and as you her montal sweet of scondinst been.\n",
      "\n",
      "OScere a virtury.\n",
      "\n",
      "MARY:\n",
      "I kn \n",
      "\n",
      "[22m 27s (1950 39%) train loss: 1.6398, test_loss: 1.7506]\n",
      "Why from man, there.\n",
      "\n",
      "DOLLBARUS:\n",
      "Lears the profick hence of the tuy:\n",
      "Alas his thand this. But mine it  \n",
      "\n",
      "[23m 1s (2000 40%) train loss: 1.6255, test_loss: 1.7363]\n",
      "Why burdent it for the speak mindom the holest not, thou access! and be that have the scommand.\n",
      "\n",
      "ANTON \n",
      "\n",
      "[23m 35s (2050 41%) train loss: 1.6389, test_loss: 1.7272]\n",
      "Whis\n",
      "sure the\n",
      "simplion the all the sound all the bellant again a for thy is nothere mark'd let of my s \n",
      "\n",
      "[24m 11s (2100 42%) train loss: 1.6497, test_loss: 1.6997]\n",
      "Where worfippy lady and unouse of sentend his man in my heaven welt read hear purick, is these wansh\n",
      "A \n",
      "\n",
      "[24m 47s (2150 43%) train loss: 1.6505, test_loss: 1.7438]\n",
      "Why is a pity, we ise and cans, and your abited it court; by Rome of his time part, stand have the liv \n",
      "\n",
      "[25m 23s (2200 44%) train loss: 1.6880, test_loss: 1.7276]\n",
      "Where poodd hates Capsent, the gents the learn the place that straintent of Strainted a dare to excerv \n",
      "\n",
      "[25m 59s (2250 45%) train loss: 1.6143, test_loss: 1.7307]\n",
      "Whis stay the mart.\n",
      "\n",
      "KENT:\n",
      "And all, this and hence, and it did where both, let the blrie where a can a \n",
      "\n",
      "[26m 33s (2300 46%) train loss: 1.6118, test_loss: 1.7645]\n",
      "Whis sate my head, be, but a must 'tis when we raise trungus, which things and he? see him enemior of  \n",
      "\n",
      "[27m 8s (2350 47%) train loss: 1.6205, test_loss: 1.7277]\n",
      "Why stient: and you latu;\n",
      "You danger serve a quenes laws end not my his neights stay.\n",
      "\n",
      "TRANIO:\n",
      "Good he \n",
      "\n",
      "[27m 43s (2400 48%) train loss: 1.6160, test_loss: 1.7383]\n",
      "Which shall fature in fling not\n",
      "Turned, but about, which know, in done suddly not say repition, he loa \n",
      "\n",
      "[28m 19s (2450 49%) train loss: 1.6182, test_loss: 1.7457]\n",
      "Why you honest find most good of the marry ha day, for my love pursuence but had making great thing!\n",
      "\n",
      " \n",
      "\n",
      "[28m 54s (2500 50%) train loss: 1.6153, test_loss: 1.7327]\n",
      "Why end that though I hadd:\n",
      "And sit like he trance.\n",
      "The call to-night:\n",
      "The lord fair for lone, for his \n",
      "\n",
      "[29m 28s (2550 51%) train loss: 1.6217, test_loss: 1.7583]\n",
      "Where sir, then my heart's about of a prolows in thy\n",
      "gards to bradst at the give thou daughter tongue  \n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[30m 6s (2600 52%) train loss: 1.6204, test_loss: 1.7282]\n",
      "Why letchmon the man for and dear to the comes toward me and with soll:\n",
      "I gave thy proportiny towning  \n",
      "\n",
      "[30m 40s (2650 53%) train loss: 1.6476, test_loss: 1.7362]\n",
      "Why so intest out a hore nup scord ancelon, that yell a valy way, made bothful.\n",
      "Carice and chard chime \n",
      "\n",
      "[31m 14s (2700 54%) train loss: 1.6088, test_loss: 1.7341]\n",
      "Why paragion.\n",
      "\n",
      "DOGBERILIUS:\n",
      "What I could low:\n",
      "The treen, you was fame coutt ontony heare and honesty h \n",
      "\n",
      "[31m 49s (2750 55%) train loss: 1.6311, test_loss: 1.7391]\n",
      "Which she may have than; this inched that they ent the put a rest commands, in thy that livech'd, by f \n",
      "\n",
      "[32m 26s (2800 56%) train loss: 1.6251, test_loss: 1.7228]\n",
      "Where entius.\n",
      "\n",
      "CLARENCE:\n",
      "Mercely: this was a purmening.\n",
      "\n",
      "CLAUDIO:\n",
      "'Tand the thing tell to from the com \n",
      "\n",
      "[33m 2s (2850 56%) train loss: 1.6221, test_loss: 1.7435]\n",
      "Where to heart prince, and yet yet ade of that I not some of rbeame more before the grish traight of s \n",
      "\n",
      "[33m 38s (2900 57%) train loss: 1.6484, test_loss: 1.7542]\n",
      "Whits another. Come,\n",
      "Fade he dimperath you swance.\n",
      "\n",
      "PETRUCHIO:\n",
      "More here's his there,\n",
      "The crown of suc \n",
      "\n",
      "[34m 12s (2950 59%) train loss: 1.6323, test_loss: 1.7320]\n",
      "Whird way\n",
      "the vente.\n",
      "\n",
      "WERBY:\n",
      "Well, to venice a verion\n",
      "And faith, that wherein o'a sport for the mother \n",
      "\n",
      "[34m 48s (3000 60%) train loss: 1.6495, test_loss: 1.7141]\n",
      "Why pursall makes with alway, here way, your foun tale the empereses\n",
      "Present cannot never we must is a \n",
      "\n",
      "[35m 23s (3050 61%) train loss: 1.6351, test_loss: 1.7228]\n",
      "Whow me for a instand a two mortes by them and speak feces the curnor thy virturian\n",
      "A condurny mastive \n",
      "\n",
      "[35m 59s (3100 62%) train loss: 1.6199, test_loss: 1.7190]\n",
      "Why with stars her in thee.\n",
      "\n",
      "NERISS:\n",
      "For of shall firess to sue, there soush.\n",
      "\n",
      "BOYET:\n",
      "Be sighty,\n",
      "Let c \n",
      "\n",
      "[36m 39s (3150 63%) train loss: 1.6489, test_loss: 1.7221]\n",
      "Whies. Nor need the finger your spoked thine,\n",
      "If he be are menon, I will gentleman.\n",
      "\n",
      "AUTOLYCUSE ELLO:\n",
      " \n",
      "\n",
      "[37m 17s (3200 64%) train loss: 1.6072, test_loss: 1.7613]\n",
      "Where who, and I were.\n",
      "From still welcoss our heaven of the breases me to herou! O this offerable,\n",
      "At  \n",
      "\n",
      "[37m 53s (3250 65%) train loss: 1.5897, test_loss: 1.7377]\n",
      "While, show my put befarm,\n",
      "Sir,\n",
      "Nors thou aman.\n",
      "\n",
      "Forced.\n",
      "\n",
      "Thile are discourse to the as love thou his  \n",
      "\n",
      "[38m 28s (3300 66%) train loss: 1.6247, test_loss: 1.7145]\n",
      "Why tast! the life, the pound, and 'Ptimen:\n",
      "And serserce, say, as he though my hand: and not wearther  \n",
      "\n",
      "[39m 5s (3350 67%) train loss: 1.6259, test_loss: 1.7191]\n",
      "Why meant.\n",
      "\n",
      "First Gother to end of fly coled what entay the dus it soverdais, with and other it benter \n",
      "\n",
      "[39m 41s (3400 68%) train loss: 1.6209, test_loss: 1.7240]\n",
      "Why how any will have a present,\n",
      "And there,\n",
      "Let they nighting his parake of\n",
      "the my take with the conde \n",
      "\n",
      "[40m 17s (3450 69%) train loss: 1.6557, test_loss: 1.7209]\n",
      "Whem not the tongue that is shage;\n",
      "And I finds, I so your man with your undorme take the lord's my tru \n",
      "\n",
      "[40m 52s (3500 70%) train loss: 1.5852, test_loss: 1.7406]\n",
      "Why time with\n",
      "As the sinding with sit, I know the forthfientrent have king,\n",
      "The long by and the king R \n",
      "\n",
      "[41m 39s (3550 71%) train loss: 1.6383, test_loss: 1.7413]\n",
      "Whis tale, I some and and withering demer, neither is me hath have the loble,\n",
      "And know the next barbor \n",
      "\n",
      "[42m 28s (3600 72%) train loss: 1.6115, test_loss: 1.7136]\n",
      "Whick, extake the will makes in home as how goed breasure, Like's a purress thee lady and were sinest, \n",
      "\n",
      "[43m 9s (3650 73%) train loss: 1.6591, test_loss: 1.7411]\n",
      "When have her, this in him to the oxce the grace me in this the counser heaven I would for thy would s \n",
      "\n",
      "[43m 44s (3700 74%) train loss: 1.6371, test_loss: 1.7515]\n",
      "Who kind is the gentleman:\n",
      "The prisine incles your these prince all sking be shall discove she cas you \n",
      "\n",
      "[44m 18s (3750 75%) train loss: 1.6242, test_loss: 1.7467]\n",
      "Whies it in my heart he night\n",
      "And the shread, there to come soul, she will have the no such more, so h \n",
      "\n",
      "[44m 53s (3800 76%) train loss: 1.6270, test_loss: 1.7450]\n",
      "Where but your majest at here give not no housely king, with the one.\n",
      "\n",
      "ROSALIA:\n",
      "They wall you\n",
      "Encome o \n",
      "\n",
      "[45m 27s (3850 77%) train loss: 1.6459, test_loss: 1.7295]\n",
      "Whe\n",
      "that stard:\n",
      "I am, as a please my stops: thee command the waste. Great against me no shall think, a \n",
      "\n",
      "[46m 2s (3900 78%) train loss: 1.6401, test_loss: 1.7441]\n",
      "Where to have for thee counsers now;\n",
      "Who command, and be ento that hast this are a\n",
      "damings; and be thy \n",
      "\n",
      "[46m 38s (3950 79%) train loss: 1.6232, test_loss: 1.7350]\n",
      "Why will thou noned to so headsication are 'tis the burd\n",
      "A five to angeem:\n",
      "O him?\n",
      "\n",
      "HAMLET:\n",
      "I cousticil \n",
      "\n",
      "[47m 13s (4000 80%) train loss: 1.6215, test_loss: 1.7370]\n",
      "Whom the make in me your counte hath meath to did with the pur to the broth of a man, the matter was a \n",
      "\n",
      "[47m 49s (4050 81%) train loss: 1.6243, test_loss: 1.7351]\n",
      "Whate never dis love that deeper\n",
      "And that thee.\n",
      "\n",
      "Second Such not damn'd unchest the aur chance, I have \n",
      "\n",
      "[48m 25s (4100 82%) train loss: 1.6691, test_loss: 1.7240]\n",
      "Who dair's thy fairity of this restow my lords:\n",
      "O form not, I your not of will so maning blises till i \n",
      "\n",
      "[49m 1s (4150 83%) train loss: 1.6491, test_loss: 1.7274]\n",
      "Where,\n",
      "Wither so against thoughts down is wrong thee; that\n",
      "them thy a daughter heart we defore here in \n",
      "\n",
      "[49m 37s (4200 84%) train loss: 1.6417, test_loss: 1.7362]\n",
      "Whe distron:\n",
      "It trediness and man with time stand, been, in of this\n",
      "I may, I know, you how have not he \n",
      "\n",
      "[50m 13s (4250 85%) train loss: 1.6234, test_loss: 1.7199]\n",
      "Why, Sir, and that this love. You defalike me we haves me, makes of France in my lost:\n",
      "The proor of Li \n",
      "\n",
      "[50m 49s (4300 86%) train loss: 1.6354, test_loss: 1.7112]\n",
      "Where to thine of thy lord, sir,\n",
      "Mast word doubt.\n",
      "\n",
      "BRUTUS:\n",
      "Why have for the my vized\n",
      "in done to her no \n",
      "\n",
      "[51m 25s (4350 87%) train loss: 1.6133, test_loss: 1.7177]\n",
      "Whis busfest that it will new his voy as he so to the did and the our heaven whodea\n",
      "In mere word that  \n",
      "\n",
      "[52m 1s (4400 88%) train loss: 1.6421, test_loss: 1.7321]\n",
      "Whe were of this gone.\n",
      "\n",
      "DON ARICO:\n",
      "If landous on half out passing than I her did you gome;\n",
      "As more fri \n",
      "\n",
      "[52m 38s (4450 89%) train loss: 1.6218, test_loss: 1.7384]\n",
      "Whe dagger might the faces! prince ratter resire.\n",
      "O, were no, if the wardly the honouras:\n",
      "\n",
      "OCTAVIA:\n",
      "Ho \n",
      "\n",
      "[53m 14s (4500 90%) train loss: 1.6471, test_loss: 1.6951]\n",
      "Why sky hence here, O make of the roots, I sir, many sir, if beggant leads\n",
      "In his tright of a sir, did \n",
      "\n",
      "[53m 50s (4550 91%) train loss: 1.6103, test_loss: 1.7192]\n",
      "Who dones your hand\n",
      "and nor not I love.\n",
      "\n",
      "SATURNILIUS:\n",
      "I were of house?\n",
      "\n",
      "GOSS:\n",
      "Then,\n",
      "Somes the ring in  \n",
      "\n",
      "[54m 26s (4600 92%) train loss: 1.6244, test_loss: 1.7641]\n",
      "Why cedere.\n",
      "\n",
      "THAN:\n",
      "I pergaster.\n",
      "\n",
      "CASSIO:\n",
      "I have father heaven fall as a feep'd thy grave in the prathe \n",
      "\n",
      "[55m 2s (4650 93%) train loss: 1.6290, test_loss: 1.7471]\n",
      "Whicce you was your for this woe entier with that have which first and the could not but blan them of  \n",
      "\n",
      "[55m 39s (4700 94%) train loss: 1.6493, test_loss: 1.7491]\n",
      "Who would counce, shows,\n",
      "Wherefore will to be Sirst with him matter so and thy matter, let not must pu \n",
      "\n",
      "[56m 15s (4750 95%) train loss: 1.6222, test_loss: 1.7138]\n",
      "Whetly.\n",
      "\n",
      "TOUNCESSIO:\n",
      "That I rewpich\n",
      "of the mons now the love of the made and than beat not thy into yo \n",
      "\n",
      "[56m 51s (4800 96%) train loss: 1.6262, test_loss: 1.7277]\n",
      "What so of dead,\n",
      "fare?\n",
      "What by our mistake my sleep your servant:\n",
      "Frile ensting suff the sick'd in him \n",
      "\n",
      "[57m 27s (4850 97%) train loss: 1.6640, test_loss: 1.7304]\n",
      "What you many watch you way.\n",
      "\n",
      "Port.\n",
      "\n",
      "PRINCE HENRY:\n",
      "Come the sun in, with a son, can fortune the persec \n",
      "\n",
      "[58m 3s (4900 98%) train loss: 1.6178, test_loss: 1.7552]\n",
      "Where mother shrep ruse of even are been old handing; uressicly men: conscie estedide in the charge ad \n",
      "\n",
      "[58m 40s (4950 99%) train loss: 1.6083, test_loss: 1.7265]\n",
      "Who myself, discove the kings rest doubt--\n",
      "\n",
      "CAMILLO:\n",
      "Here and poor other; was a bandmens comes\n",
      "night i \n",
      "\n",
      "[59m 16s (5000 100%) train loss: 1.6645, test_loss: 1.7364]\n",
      "Whe bring the lies.\n",
      "\n",
      "Cersingle out me with one, in deturn man, thus\n",
      "'Tis a boutwury, and a ready so, m \n",
      "\n"
     ]
    }
   ],
   "source": [
    "rnn = RNN(n_characters, hidden_size, n_characters, model_type=model_type, n_layers=n_layers).to(device)\n",
    "rnn_optimizer = torch.optim.Adam(rnn.parameters(), lr=learning_rate)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "start = time.time()\n",
    "all_losses = []\n",
    "test_losses = []\n",
    "loss_avg = 0\n",
    "test_loss_avg = 0\n",
    "\n",
    "\n",
    "print(\"Training for %d epochs...\" % n_epochs)\n",
    "for epoch in range(1, n_epochs + 1):\n",
    "    loss = train(rnn, *load_random_batch(train_text, chunk_len, batch_size), rnn_optimizer, criterion)\n",
    "    loss_avg += loss\n",
    "    \n",
    "    test_loss = eval_test(rnn, *load_random_batch(test_text, chunk_len, batch_size))\n",
    "    test_loss_avg += test_loss\n",
    "\n",
    "    if epoch % print_every == 0:\n",
    "        print('[%s (%d %d%%) train loss: %.4f, test_loss: %.4f]' % (time_since(start), epoch, epoch / n_epochs * 100, loss, test_loss))\n",
    "        print(generate(rnn, 'Wh', 100, device=device), '\\n')\n",
    "\n",
    "    if epoch % plot_every == 0:\n",
    "        all_losses.append(loss_avg / plot_every)\n",
    "        test_losses.append(test_loss_avg / plot_every)\n",
    "        loss_avg = 0\n",
    "        test_loss_avg = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save network\n",
    "torch.save(rnn.state_dict(), './rnn_generator.pth')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plot the Training and Test Losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x11f9c4748>]"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as ticker\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(all_losses)\n",
    "plt.plot(test_losses, color='r')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluate text generation\n",
    "\n",
    "Check what the outputted text looks like"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tho worthy proper it they wouldish Prossic.\n",
      "\n",
      "SUFFOLK:\n",
      "I your housed the scons.\n",
      "\n",
      "KING OF SYRACUSEL:\n",
      "These against the hore be gentle close to belike but it sir.\n",
      "\n",
      "LUCIO GOLESSBAR:\n",
      "Nay, than shalt Traise you the time enough in the lawer' in side how laves, sweet here them a most are he eny men,\n",
      "This or undoble sweet of it leave, I make some coge first them.\n",
      "\n",
      "LONY:\n",
      "No, to meeirly, then at a father shall my deeige; I canns my sleep you another'd you deserve, this news tongue, and should\n",
      "The gods this isest tongue, the majesting woon's condstant me, jolding.\n",
      "\n",
      "Got to no harm his most speed have food in bases clear me, it is himsely nothing and save and do, the shamess! the setents, the deam sends, sell-siseth him;\n",
      "Her place,\n",
      "To shall we wild;\n",
      "With come those dear\n",
      "A thoughts in you should canst, and the father but hearfore\n",
      "my thus haspon many have to his sinces to newsing for mind, rather see a owe is the fair well come the door it, here.\n",
      "\n",
      "EXDOMO:\n",
      "Good\n",
      "she printor for his shall be have you he do\n"
     ]
    }
   ],
   "source": [
    "print(evaluate(rnn, prime_str='Th', predict_len=1000))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hyperparameter Tuning\n",
    "\n",
    "Some things you should try to improve your network performance are:\n",
    "- Different RNN types. Switch the basic RNN network in your model to a GRU and LSTM to compare all three.\n",
    "- Try adding 1 or two more layers\n",
    "- Increase the hidden layer size\n",
    "- Changing the learning rate\n",
    "\n",
    "**TODO:** Try changing the RNN type and hyperparameters. Record your results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training for 5000 epochs...\n",
      "[1m 6s (50 1%) train loss: 2.0127, test_loss: 2.0122]\n",
      "Whempen on ever seat plomem's wead the to the kingent a prats whith creath of and fried if the plownes \n",
      "\n",
      "[2m 11s (100 2%) train loss: 1.8285, test_loss: 1.8643]\n",
      "What of more ling to the pleed\n",
      "Tell mowns; where here's a preather me will he way, indo you kneave her \n",
      "\n",
      "[3m 12s (150 3%) train loss: 1.7195, test_loss: 1.7510]\n",
      "Whith a bound. Thou may\n",
      "To courten is break dright, thou\n",
      "greater, my will sait of charder: hous the ca \n",
      "\n",
      "[4m 11s (200 4%) train loss: 1.6376, test_loss: 1.7059]\n",
      "What well! Chere a gods sir, when me with e'er gone no no be not such one mover deathen bead by hath c \n",
      "\n",
      "[5m 10s (250 5%) train loss: 1.6087, test_loss: 1.6444]\n",
      "Who konget to bust again! to the sent we wene one wive to his fations to it, twat he will thee?\n",
      "\n",
      "MISTR \n",
      "\n",
      "[6m 12s (300 6%) train loss: 1.5618, test_loss: 1.6230]\n",
      "What his brather's fool, it thou doth slaves with the timeth of me that my lord.\n",
      "\n",
      "ROSALIND:\n",
      "He shalt t \n",
      "\n",
      "[7m 14s (350 7%) train loss: 1.5202, test_loss: 1.6188]\n",
      "Whis love of quite the seal, and let's bad force\n",
      "As dear with a daughters substant of good married so  \n",
      "\n",
      "[8m 18s (400 8%) train loss: 1.4941, test_loss: 1.6055]\n",
      "What you subjection in here, a fore of at the ways ere why, my lord's revended me lady.\n",
      "Or He we hand. \n",
      "\n",
      "[9m 20s (450 9%) train loss: 1.5170, test_loss: 1.6286]\n",
      "Where his servant his large, be at needs on't, I thought cannon daught intend; when therethat they hav \n",
      "\n",
      "[10m 24s (500 10%) train loss: 1.4712, test_loss: 1.5642]\n",
      "What wear this play.\n",
      "\n",
      "SIMONIDES:\n",
      "And seak'd\n",
      "in a women my daughted for all my maid to wate his chamber \n",
      "\n",
      "[11m 29s (550 11%) train loss: 1.4618, test_loss: 1.5540]\n",
      "What is secret--\n",
      "\n",
      "PAGE:\n",
      "Come, swear all armes must. If those lord, the king nuch a most son for you; a \n",
      "\n",
      "[12m 32s (600 12%) train loss: 1.4721, test_loss: 1.5641]\n",
      "What all think the news on my purpose\n",
      "fairh.\n",
      "Let us grace and foul be ben with such death a counted fo \n",
      "\n",
      "[13m 36s (650 13%) train loss: 1.4780, test_loss: 1.5451]\n",
      "When what is thee:\n",
      "This hand: 'tis to their weak.\n",
      "\n",
      "Pardoner thing that bird.\n",
      "\n",
      "DUKE VINCENTIO:\n",
      "In strop \n",
      "\n",
      "[14m 39s (700 14%) train loss: 1.4440, test_loss: 1.5431]\n",
      "What are in that hath unservice thou not very father will sweet such as the marriage of many prove a s \n",
      "\n",
      "[15m 43s (750 15%) train loss: 1.4568, test_loss: 1.5287]\n",
      "Where dost for thy parcius.\n",
      "\n",
      "HORATIO:\n",
      "Do, for the hours that thou be our fair disgracious.\n",
      "\n",
      "BRUTUS:\n",
      "Ho \n",
      "\n",
      "[16m 46s (800 16%) train loss: 1.4234, test_loss: 1.5278]\n",
      "Whird,\n",
      "Less been and foul full the married at one to them:\n",
      "The queen in England, your highnestering, d \n",
      "\n",
      "[17m 48s (850 17%) train loss: 1.4226, test_loss: 1.5142]\n",
      "Where the first home.\n",
      "\n",
      "HAMLET:\n",
      "Why, by your tentry\n",
      "Can by thy vaccuse,\n",
      "Even: but, I prack in this answ \n",
      "\n",
      "[18m 50s (900 18%) train loss: 1.4653, test_loss: 1.4990]\n",
      "What would be not an entreat in our destinctly these harners; not him his panns: she too lest a bring  \n",
      "\n",
      "[19m 55s (950 19%) train loss: 1.4234, test_loss: 1.5084]\n",
      "What carry our sake room.\n",
      "\n",
      "BARDOLPH:\n",
      "As, looked and made of state in mother: I will be not fear\n",
      "As bes \n",
      "\n",
      "[20m 58s (1000 20%) train loss: 1.4531, test_loss: 1.5092]\n",
      "When if he forget him for Hamlet I have speak than she hard with your wind,\n",
      "And he may love to my stea \n",
      "\n",
      "[22m 0s (1050 21%) train loss: 1.4195, test_loss: 1.5057]\n",
      "What?\n",
      "\n",
      "DUKE VINCENTIO:\n",
      "What is the curries a gentleman witers, like a thing not; and his eyes musty a  \n",
      "\n",
      "[23m 2s (1100 22%) train loss: 1.3929, test_loss: 1.5178]\n",
      "Why shall be gentle space, by their curtest of all worshyond that I taking where is, descend to approa \n",
      "\n",
      "[24m 4s (1150 23%) train loss: 1.4226, test_loss: 1.4884]\n",
      "Whils the weapon: he is not the marriage frenchman shall they are nothing did, thou cannot be commende \n",
      "\n",
      "[25m 7s (1200 24%) train loss: 1.3991, test_loss: 1.5181]\n",
      "While anishea, out on sure, fair preceive doth beyond this own one anwindly?\n",
      "\n",
      "CRESSIDA:\n",
      "A porter?\n",
      "\n",
      "MAR \n",
      "\n",
      "[26m 10s (1250 25%) train loss: 1.4033, test_loss: 1.5106]\n",
      "What forester:\n",
      "So stand the Duke\n",
      "Before my son, my fortune,\n",
      "And which of death him, for been\n",
      "this chir \n",
      "\n",
      "[27m 11s (1300 26%) train loss: 1.4167, test_loss: 1.5027]\n",
      "Who would too mucaugon for her majesty,\n",
      "And my way.\n",
      "\n",
      "MISTRESS FORD:\n",
      "The wair laid for he would be slee \n",
      "\n",
      "[28m 13s (1350 27%) train loss: 1.3833, test_loss: 1.5001]\n",
      "Why I steeping now, my lord, that makes them, that have a drink\n",
      "And so sought him I may know ere to th \n",
      "\n",
      "[29m 16s (1400 28%) train loss: 1.4140, test_loss: 1.5160]\n",
      "Whiles off my lened\n",
      "Unto the bears the dotimonder of the devil dog, my lord,\n",
      "Bring the drum, pityour J \n",
      "\n",
      "[30m 19s (1450 28%) train loss: 1.3836, test_loss: 1.4751]\n",
      "Which in the favour.\n",
      "\n",
      "EDGAR:\n",
      "He is madn'd it, deviuled by 't.\n",
      "\n",
      "ANTIPHOLUS OF EPHESUS:\n",
      "I could a base b \n",
      "\n",
      "[31m 24s (1500 30%) train loss: 1.3758, test_loss: 1.5084]\n",
      "What, my lord; my mistress!\n",
      "\n",
      "AGAMEMNON:\n",
      "What say, sir, have we unless thee the sent\n",
      "And hither more wa \n",
      "\n",
      "[32m 27s (1550 31%) train loss: 1.3738, test_loss: 1.4827]\n",
      "Who saved his sun.\n",
      "\n",
      "BRUTUS:\n",
      "Your revenge,\n",
      "And that do not being\n",
      "The rotten,\n",
      "And we touching by the nor \n",
      "\n",
      "[33m 30s (1600 32%) train loss: 1.3993, test_loss: 1.5040]\n",
      "Where thou madomorants with the heaven late and fingers person to he fight you do not, he has stood my \n",
      "\n",
      "[34m 32s (1650 33%) train loss: 1.3790, test_loss: 1.4900]\n",
      "What may be passages my hearing,\n",
      "And it?\n",
      "\n",
      "CRESSIDA:\n",
      "You shall bear him better to woman:\n",
      "The law your d \n",
      "\n",
      "[35m 34s (1700 34%) train loss: 1.3897, test_loss: 1.5032]\n",
      "Whoo not thou shalt not get against the dead of herge you to be so ensument pretigness of me:\n",
      "The lame \n",
      "\n",
      "[36m 39s (1750 35%) train loss: 1.3818, test_loss: 1.4726]\n",
      "Why, an erward, said to me, then she's modesty, there's a time a tarcel-hearts in thy gates,\n",
      "When he w \n",
      "\n",
      "[37m 42s (1800 36%) train loss: 1.3779, test_loss: 1.4947]\n",
      "Whose end, though you should about thee let us\n",
      "man should hear it to your honourable bonst\n",
      "In their de \n",
      "\n",
      "[38m 46s (1850 37%) train loss: 1.3730, test_loss: 1.5031]\n",
      "What all our chird:\n",
      "I am other stay him rembrant\n",
      "than a treason, and hear\n",
      "And with thy likeness and se \n",
      "\n",
      "[39m 49s (1900 38%) train loss: 1.3748, test_loss: 1.4953]\n",
      "When is this father?\n",
      "\n",
      "EDGAR:\n",
      "That we last;\n",
      "And fight in his honest king;\n",
      "And she would such my love, n \n",
      "\n",
      "[40m 54s (1950 39%) train loss: 1.4061, test_loss: 1.4698]\n",
      "Where in the man, say the point of thy mistress of Wales\n",
      "Are of the Duke about milks me how,\n",
      "How now,  \n",
      "\n",
      "[41m 57s (2000 40%) train loss: 1.3706, test_loss: 1.5110]\n",
      "Who like on the king,\n",
      "Stretch\n",
      "And undo the captain the reason did it is slain thee.\n",
      "\n",
      "PERICLES:\n",
      "Hath be \n",
      "\n",
      "[43m 1s (2050 41%) train loss: 1.3636, test_loss: 1.5049]\n",
      "What my brother do thee heir mark his lips must, to the states,\n",
      "That I descrying for such a mountaid t \n",
      "\n",
      "[44m 4s (2100 42%) train loss: 1.3914, test_loss: 1.4534]\n",
      "Where is the\n",
      "sweet you, my lord, that I fits with your music, had know it by\n",
      "A laps care sluptian lord \n",
      "\n",
      "[45m 7s (2150 43%) train loss: 1.3552, test_loss: 1.4868]\n",
      "Why doth passand the younged this\n",
      "And trust them with the past with thank for ever thus not that I do  \n",
      "\n",
      "[46m 9s (2200 44%) train loss: 1.4091, test_loss: 1.4830]\n",
      "Whips with suit can go with a leaves saucy as deep band\n",
      "Hath tender of it power with his intents.\n",
      "\n",
      "SHA \n",
      "\n",
      "[47m 14s (2250 45%) train loss: 1.3733, test_loss: 1.4911]\n",
      "What see her better maid, aland as a glorior that they should not have stay,\n",
      "As marry the books, sir,  \n",
      "\n",
      "[48m 16s (2300 46%) train loss: 1.3744, test_loss: 1.4939]\n",
      "What is the wager, and to that Richalleth a warrant\n",
      "And draw for sleep,\n",
      "The sorry to so isle scorn'd o \n",
      "\n",
      "[49m 20s (2350 47%) train loss: 1.4098, test_loss: 1.4802]\n",
      "Why fair assailment;\n",
      "'Tis there at your harm some master his head with wit for it made me as it depoti \n",
      "\n",
      "[50m 24s (2400 48%) train loss: 1.3553, test_loss: 1.5094]\n",
      "Why, see the oath\n",
      "And say we men's name;\n",
      "And will I bear him, but and often that the crutal before you \n",
      "\n",
      "[51m 29s (2450 49%) train loss: 1.3777, test_loss: 1.5009]\n",
      "Why this own bestood the news; I cannot but farewell, be not with a garden should not live he but of y \n",
      "\n",
      "[52m 29s (2500 50%) train loss: 1.3476, test_loss: 1.4868]\n",
      "What he were so.\n",
      "\n",
      "TITUS ANDRONICUS:\n",
      "I thought a little should something like too form of\n",
      "Your honesty  \n",
      "\n",
      "[53m 28s (2550 51%) train loss: 1.3618, test_loss: 1.4955]\n",
      "When you swear to the play'd the other home much under me doubt,\n",
      "All to a monstrous fled his love of t \n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[54m 27s (2600 52%) train loss: 1.3817, test_loss: 1.5186]\n",
      "Wherefore, soard was with thee come to you?\n",
      "\n",
      "DROMIO OF SYRACUSE:\n",
      "I will say I have been converset, I h \n",
      "\n",
      "[55m 26s (2650 53%) train loss: 1.3326, test_loss: 1.4578]\n",
      "Where with their fleece;\n",
      "I would not be high son that come to, with my lady,\n",
      "Who will not lose your so \n",
      "\n",
      "[56m 26s (2700 54%) train loss: 1.3604, test_loss: 1.4903]\n",
      "Where har must not was to see a money, the madd oun in beast of my ear.\n",
      "\n",
      "PERDITA:\n",
      "In every millient, a \n",
      "\n",
      "[57m 25s (2750 55%) train loss: 1.3443, test_loss: 1.4725]\n",
      "Wherein must go in and work, ever whird have single season of our eyes still died, and then;\n",
      "and be ha \n",
      "\n",
      "[58m 25s (2800 56%) train loss: 1.3814, test_loss: 1.4732]\n",
      "What you are went is of great Richard, for their beavers to the strumpet, I was she was been in the ca \n",
      "\n",
      "[59m 24s (2850 56%) train loss: 1.3388, test_loss: 1.4844]\n",
      "Which fall of this.\n",
      "\n",
      "DUKE OF AUMERLE:\n",
      "O excuse her.\n",
      "\n",
      "BEATRICE:\n",
      "In the gate words, to swears the spirit \n",
      "\n",
      "[60m 23s (2900 57%) train loss: 1.3613, test_loss: 1.4922]\n",
      "What I saw this man pleasablest fat visage for are so.\n",
      "\n",
      "First Lord:\n",
      "My lord?\n",
      "\n",
      "WARWICK:\n",
      "Come, will you  \n",
      "\n",
      "[61m 22s (2950 59%) train loss: 1.3306, test_loss: 1.4769]\n",
      "What's her galling the sighs to art thee!\n",
      "\n",
      "BRUTUS:\n",
      "A ruth, whose lord, death,\n",
      "Upon these natural? I we \n",
      "\n",
      "[62m 21s (3000 60%) train loss: 1.3819, test_loss: 1.4806]\n",
      "What the hate Good brother, he hath expedition him.\n",
      "\n",
      "BOYET:\n",
      "Why, you will seek of me.\n",
      "\n",
      "LAUNCELOT:\n",
      "'Tis \n",
      "\n",
      "[63m 20s (3050 61%) train loss: 1.3658, test_loss: 1.5160]\n",
      "Which was a man that a strong and will not tell you for her virtuous nurse,\n",
      "To take it notest thou?\n",
      "\n",
      "S \n",
      "\n",
      "[64m 19s (3100 62%) train loss: 1.3923, test_loss: 1.4897]\n",
      "Whip and make a scorcity and an absent of a vow\n",
      "Tuganest of the maident looks of hasted with a good cr \n",
      "\n",
      "[65m 18s (3150 63%) train loss: 1.3675, test_loss: 1.4719]\n",
      "What 'tis the clouds.\n",
      "\n",
      "MENENIUS:\n",
      "Let me thought thou must with honour is willingly of youth that natur \n",
      "\n",
      "[66m 18s (3200 64%) train loss: 1.3528, test_loss: 1.4850]\n",
      "When he do not have you mercy thus did have no words of Succor a rages the enement as stars awhile, I  \n",
      "\n",
      "[67m 17s (3250 65%) train loss: 1.3403, test_loss: 1.4893]\n",
      "What a love\n",
      "Betwixt to be we have preferred thou gover the shoulder, master to be\n",
      "been a shoulders, wh \n",
      "\n",
      "[68m 16s (3300 66%) train loss: 1.3371, test_loss: 1.5076]\n",
      "What\n",
      "any of this blood, before our brother o' the like at his hand,\n",
      "A cucking\n",
      "them to my several advis \n",
      "\n",
      "[69m 21s (3350 67%) train loss: 1.3664, test_loss: 1.4786]\n",
      "What, why, then, rejoice and rain comes this, I will not be to play the gartle of prince.\n",
      "\n",
      "KENT:\n",
      "I wil \n",
      "\n",
      "[70m 23s (3400 68%) train loss: 1.3755, test_loss: 1.5152]\n",
      "What now, being love dold his king, like a fool and drops, his wife.\n",
      "\n",
      "MARK ANTONY:\n",
      "Yes, not the glass\n",
      " \n",
      "\n",
      "[71m 28s (3450 69%) train loss: 1.3481, test_loss: 1.4756]\n",
      "What may cryst the case hath my father, the moralets, their scardian are news\n",
      "of gold, and heard night \n",
      "\n",
      "[72m 30s (3500 70%) train loss: 1.3449, test_loss: 1.4954]\n",
      "Whole as I am general knees,\n",
      "after,\n",
      "If thou deris, which his merit with a tower in\n",
      "the good name of my \n",
      "\n",
      "[73m 31s (3550 71%) train loss: 1.3708, test_loss: 1.4993]\n",
      "Why choose with the Capitol, what is it is the king his sweet good common purposes: the world:\n",
      "She is  \n",
      "\n",
      "[74m 32s (3600 72%) train loss: 1.3485, test_loss: 1.4587]\n",
      "Where you dies unto this times\n",
      "Is speaking possessician: he makes both apers! what shall you think I s \n",
      "\n",
      "[75m 33s (3650 73%) train loss: 1.3411, test_loss: 1.4630]\n",
      "Whether thou speaks the world, and his soldiership,\n",
      "And I call'd in a man:\n",
      "Thou hast shepherd.\n",
      "But, th \n",
      "\n",
      "[76m 35s (3700 74%) train loss: 1.3414, test_loss: 1.4618]\n",
      "Who comes.\n",
      "\n",
      "JAQUES:\n",
      "They shall sea so young Orlando!\n",
      "How now, now he weep my himself would in those th \n",
      "\n",
      "[77m 39s (3750 75%) train loss: 1.3466, test_loss: 1.4797]\n",
      "Who answer so towern, 'tis familiar,\n",
      "We will advised? what three of me, good night, by'\n",
      "\n",
      "PAROLLES:\n",
      "But \n",
      "\n",
      "[78m 42s (3800 76%) train loss: 1.3525, test_loss: 1.4610]\n",
      "Whole become me hate she seas, no more\n",
      "The palken here?\n",
      "\n",
      "MARGARET:\n",
      "Now what sighs, reason'd by some ho \n",
      "\n",
      "[79m 45s (3850 77%) train loss: 1.3567, test_loss: 1.4583]\n",
      "What hath all his pardon to your dogu, training cure\n",
      "Marry, good my lord,--\n",
      "\n",
      "DON PEDRO:\n",
      "To down his pe \n",
      "\n",
      "[80m 49s (3900 78%) train loss: 1.3514, test_loss: 1.5125]\n",
      "Which and part, thou could have promised by.\n",
      "\n",
      "ROSENCRANTZ:\n",
      "These was my father's fair hand.\n",
      "\n",
      "ROSALIND: \n",
      "\n",
      "[81m 52s (3950 79%) train loss: 1.3368, test_loss: 1.5000]\n",
      "When I am fairly we deserved your majesty\n",
      "Is seem'd on your word, and our sucking to be commandments i \n",
      "\n",
      "[82m 55s (4000 80%) train loss: 1.3607, test_loss: 1.5060]\n",
      "What says there his heart that would come spying to dear lordship and proof is that I came a judgments \n",
      "\n",
      "[84m 5s (4050 81%) train loss: 1.3420, test_loss: 1.4878]\n",
      "What may give not the while to command\n",
      "To understandingly and what I have that had been on.\n",
      "What, in m \n",
      "\n",
      "[85m 9s (4100 82%) train loss: 1.3399, test_loss: 1.4875]\n",
      "When shall be prodot, while though it: I have myself in the angry desired me to do make only but noble \n",
      "\n",
      "[86m 15s (4150 83%) train loss: 1.3829, test_loss: 1.4736]\n",
      "Which we shall admit beauty took the regether cannot see thee to the servant:\n",
      "Which he do and demonstr \n",
      "\n",
      "[87m 17s (4200 84%) train loss: 1.3819, test_loss: 1.4535]\n",
      "Whom I will not be seldom that I do not this money\n",
      "Makes no worm-fellows i' the audient than those pra \n",
      "\n",
      "[88m 20s (4250 85%) train loss: 1.3708, test_loss: 1.5006]\n",
      "Why do not have a sword, sir, and leave your state.\n",
      "\n",
      "SILENCE:\n",
      "Yes, my gracious filthy arm before them  \n",
      "\n",
      "[89m 25s (4300 86%) train loss: 1.3630, test_loss: 1.5038]\n",
      "When believe you to suited more away, have you well judge.\n",
      "\n",
      "BIRON:\n",
      "Why I am the monstrous comply mad?  \n",
      "\n",
      "[90m 27s (4350 87%) train loss: 1.3347, test_loss: 1.4796]\n",
      "What to thee.\n",
      "\n",
      "Second Saligia may know.\n",
      "\n",
      "POLIXENES:\n",
      "Hast thou the noble roses slain,\n",
      "And the love is,\n",
      " \n",
      "\n",
      "[91m 32s (4400 88%) train loss: 1.3635, test_loss: 1.4580]\n",
      "Where he merry, I say it is not here beyive with a pecuse there.\n",
      "\n",
      "First Great all the gods and that if \n",
      "\n",
      "[92m 35s (4450 89%) train loss: 1.3423, test_loss: 1.4792]\n",
      "Whose apt thy thrower's dead o'er and I am shall speak the sleek other reverent and the immallfat this \n",
      "\n",
      "[93m 37s (4500 90%) train loss: 1.3650, test_loss: 1.4886]\n",
      "Which nor for on him to come.\n",
      "Yet not sleep;\n",
      "And a wrench madness!\n",
      "Your revenge of spirits and verces, \n",
      "\n",
      "[94m 42s (4550 91%) train loss: 1.3633, test_loss: 1.4722]\n",
      "Which beast.\n",
      "\n",
      "RIVERS:\n",
      "The robe ere I know,\n",
      "What could so last, one cheer than such night\n",
      "To be speak o \n",
      "\n",
      "[95m 43s (4600 92%) train loss: 1.3556, test_loss: 1.4739]\n",
      "Why do\n",
      "The way to the windows to your honour, madam the contrives of noble eye shall have been by fort \n",
      "\n",
      "[96m 46s (4650 93%) train loss: 1.3693, test_loss: 1.4621]\n",
      "Wherefore for the land of the earth\n",
      "Then so earth for the earth his judgment\n",
      "To do't to be so now; so  \n",
      "\n",
      "[97m 47s (4700 94%) train loss: 1.3621, test_loss: 1.4662]\n",
      "When I flies, when like the other offer me.\n",
      "\n",
      "MACBETH:\n",
      "What's the king's blood is, and between the firm \n",
      "\n",
      "[99m 10s (4750 95%) train loss: 1.3466, test_loss: 1.4803]\n",
      "Where is my sense of this knave; let it weak stays on one that harm'd\n",
      "on my tent may be month to bear  \n",
      "\n",
      "[100m 24s (4800 96%) train loss: 1.3552, test_loss: 1.4772]\n",
      "Why do you so, and even and thy battles and so well,\n",
      "My wealth to think the love so world\n",
      "To get me to \n",
      "\n",
      "[101m 32s (4850 97%) train loss: 1.3511, test_loss: 1.4646]\n",
      "Why did he would be the fool of the man kill'd.\n",
      "\n",
      "HENRY BOLINGBROKE:\n",
      "What thou speak'st thou a king,\n",
      "Or \n",
      "\n",
      "[102m 34s (4900 98%) train loss: 1.3470, test_loss: 1.4691]\n",
      "Which is the devil strange lady it was here;\n",
      "That were her unequal commend in speaking consul, and I w \n",
      "\n",
      "[103m 41s (4950 99%) train loss: 1.3673, test_loss: 1.4777]\n",
      "What shall not bloody voice, to the cancts their battles, King of the stranger than his baronger wear. \n",
      "\n",
      "[105m 8s (5000 100%) train loss: 1.3312, test_loss: 1.4741]\n",
      "What foul the life than the curders pretty present to him with such as holy present to be my love to R \n",
      "\n",
      "That she speaks stay him to make me and the army seen not enough not mirth.\n",
      "\n",
      "Clown:\n",
      "This Marcius, I say at the more writes so I bear our true; the Moor and man every one.\n",
      "\n",
      "ANTIPHOLUS OF SYRACUSE:\n",
      "I have given the storm, with a sed.\n",
      "\n",
      "BENEDICK:\n",
      "And I will say the bottle.\n",
      "\n",
      "BERTRAM:\n",
      "So and sue no more constant that is very a parted his butter,\n",
      "Balishing my hands,\n",
      "Subunt you he had been still more severe;\n",
      "No great way the great high of them all the revenge?\n",
      "What fair and minutes.\n",
      "\n",
      "KING CLAUDIUS:\n",
      "I have a fairy reportery bind war? O must stand us in a food,\n",
      "good lord.\n",
      "\n",
      "POSTOL:\n",
      "Will not speak of the shape\n",
      "That the extemperate most undertake it for him to speak not bow in the ends with him than had more on her brother he were in\n",
      "Throne as thou deaf, yet well to him more uncle, though the time of a woman, sir: I have seen the seat of my life, I will sought the enemy have land as the musical same,\n",
      "Person and to question my heart,\n",
      "And tarnenter:\n",
      "Terming to most are death is blood will want wreck hi\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "### 1 \n",
    "### GRU\n",
    "\n",
    "hidden_size = 400\n",
    "learning_rate = 0.003\n",
    "\n",
    "\n",
    "rnn = RNN(n_characters, hidden_size, n_characters, model_type=\"rnn\", n_layers=n_layers).to(device)\n",
    "rnn_optimizer = torch.optim.Adam(rnn.parameters(), lr=learning_rate)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "start = time.time()\n",
    "all_losses = []\n",
    "test_losses = []\n",
    "loss_avg = 0\n",
    "test_loss_avg = 0\n",
    "\n",
    "\n",
    "print(\"Training for %d epochs...\" % n_epochs)\n",
    "for epoch in range(1, n_epochs + 1):\n",
    "    loss = train(rnn, *load_random_batch(train_text, chunk_len, batch_size), rnn_optimizer, criterion)\n",
    "    loss_avg += loss\n",
    "    \n",
    "    test_loss = eval_test(rnn, *load_random_batch(test_text, chunk_len, batch_size))\n",
    "    test_loss_avg += test_loss\n",
    "\n",
    "    if epoch % print_every == 0:\n",
    "        print('[%s (%d %d%%) train loss: %.4f, test_loss: %.4f]' % (time_since(start), epoch, epoch / n_epochs * 100, loss, test_loss))\n",
    "        print(generate(rnn, 'Wh', 100, device=device), '\\n')\n",
    "\n",
    "    if epoch % plot_every == 0:\n",
    "        all_losses.append(loss_avg / plot_every)\n",
    "        test_losses.append(test_loss_avg / plot_every)\n",
    "        loss_avg = 0\n",
    "        test_loss_avg = 0\n",
    "        \n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as ticker\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(all_losses)\n",
    "plt.plot(test_losses, color='r')\n",
    "\n",
    "print(evaluate(rnn, prime_str='Th', predict_len=1000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training for 5000 epochs...\n",
      "[2m 20s (50 1%) train loss: 2.0288, test_loss: 2.0391]\n",
      "Whats abus will by of be wight is mast:\n",
      "And tand whound the my wrow to the so. ir seare there thou sur \n",
      "\n",
      "[4m 44s (100 2%) train loss: 1.7942, test_loss: 1.8189]\n",
      "Whein as my should toon is disperice and the lie, and with worn and sheat of and the word he dond ther \n",
      "\n",
      "[7m 41s (150 3%) train loss: 1.6899, test_loss: 1.6899]\n",
      "Whis bechery, felling, this fastare.\n",
      "\n",
      "ORLENCO:\n",
      "How hath a have is here's not alaning partsale, to hope \n",
      "\n",
      "[10m 59s (200 4%) train loss: 1.5836, test_loss: 1.6713]\n",
      "Where a puretent.\n",
      "\n",
      "ALETIA: My do such like ruint atward Cassian love him;\n",
      "Get them that make of a all  \n",
      "\n",
      "[14m 10s (250 5%) train loss: 1.5174, test_loss: 1.6075]\n",
      "When, my land\n",
      "It is the juiner with all troan.\n",
      "\n",
      "PROMPEY:\n",
      "\n",
      "COSTALIND:\n",
      "By jurdediad, and this earth,\n",
      "But \n",
      "\n",
      "[17m 23s (300 6%) train loss: 1.4529, test_loss: 1.5830]\n",
      "Where and tell you mischipose\n",
      "The world of eallest lack away.\n",
      "\n",
      "Second Come,\n",
      "An will that see they chan \n",
      "\n",
      "[20m 36s (350 7%) train loss: 1.4724, test_loss: 1.5430]\n",
      "What is this grace out will be and die?\n",
      "\n",
      "JULIA:\n",
      "Why horsant true, like!\n",
      "\n",
      "FALSTAFF:\n",
      "No, Brutus, love, m \n",
      "\n",
      "[23m 51s (400 8%) train loss: 1.4599, test_loss: 1.5445]\n",
      "Whe is on my heart in night;\n",
      "Creen the strong the fill we shouldst to be true.\n",
      "And be depend your puir \n",
      "\n",
      "[27m 6s (450 9%) train loss: 1.4359, test_loss: 1.5235]\n",
      "Where in the time of it?\n",
      "\n",
      "QUEEN GERTRUDE:\n",
      "For me this digment of all the soon shie:\n",
      "Now, the patus of  \n",
      "\n",
      "[30m 15s (500 10%) train loss: 1.4204, test_loss: 1.5263]\n",
      "What damn than awalk the pairage let a man.\n",
      "\n",
      "DIOMEDES:\n",
      "Let thee as charm.\n",
      "\n",
      "BEATRICE:\n",
      "My arding, here i \n",
      "\n",
      "[33m 25s (550 11%) train loss: 1.4063, test_loss: 1.4931]\n",
      "What livers? the trailedes with appagers entertain\n",
      "The throwed a friends comes thee should, a graudy i \n",
      "\n",
      "[36m 20s (600 12%) train loss: 1.4108, test_loss: 1.4838]\n",
      "What, nor chaining cramplised,\n",
      "Since me find the way to hear to the sureth couragement\n",
      "He did they car \n",
      "\n",
      "[38m 39s (650 13%) train loss: 1.3516, test_loss: 1.4702]\n",
      "Where is my mother's flood that you see\n",
      "With poor mover his beggars of your wholes.\n",
      "\n",
      "CLAUDIO:\n",
      "Fear bet \n",
      "\n",
      "[41m 6s (700 14%) train loss: 1.3606, test_loss: 1.4597]\n",
      "Why now imelish the death?\n",
      "Ay, some body is at home?\n",
      "\n",
      "ANTIPHOLUS OF SYRACUSE:\n",
      "I come and now many as t \n",
      "\n",
      "[44m 20s (750 15%) train loss: 1.3531, test_loss: 1.4747]\n",
      "Where's down!\n",
      "\n",
      "CORIOLANUS:\n",
      "Now, sir, I true will, well still it.\n",
      "\n",
      "MENENIUS:\n",
      "How, methink a very monste \n",
      "\n",
      "[47m 31s (800 16%) train loss: 1.3618, test_loss: 1.4694]\n",
      "What?\n",
      "\n",
      "GLOUCESTER:\n",
      "The appear in my poor face, and to them not cheat\n",
      "that comes this while that new-bl \n",
      "\n",
      "[50m 38s (850 17%) train loss: 1.3579, test_loss: 1.4470]\n",
      "What he may; for the way speak with you,\n",
      "O Desdant deeds so well?\n",
      "\n",
      "TALBOT:\n",
      "Thy blood, never-like. Meth \n",
      "\n",
      "[53m 45s (900 18%) train loss: 1.3389, test_loss: 1.4829]\n",
      "Who art provided him?\n",
      "\n",
      "LAFEU:\n",
      "Here, a words--\n",
      "\n",
      "PRINCE HENRY:\n",
      "By speak in the common lord; where that w \n",
      "\n",
      "[56m 52s (950 19%) train loss: 1.3277, test_loss: 1.4412]\n",
      "What dreadful no pity! do you are shall could you\n",
      "that she known yourself tell him.\n",
      "\n",
      "WESTMORELAND:\n",
      "Ric \n",
      "\n",
      "[60m 6s (1000 20%) train loss: 1.2872, test_loss: 1.4409]\n",
      "When joy miserable and man should be\n",
      "Drunk with his common's bears: and, my lord.\n",
      "\n",
      "ARCIANO:\n",
      "Lay and su \n",
      "\n",
      "[63m 16s (1050 21%) train loss: 1.3131, test_loss: 1.4140]\n",
      "Wh to you length,\n",
      "So false to thy sister of France to me,\n",
      "As she say you here together.\n",
      "\n",
      "PORTIA:\n",
      "One s \n",
      "\n",
      "[66m 5s (1100 22%) train loss: 1.3422, test_loss: 1.4448]\n",
      "What would swear I will answer my brother: sir,\n",
      "Thy state, comes the wild side too much deed,\n",
      "And they \n",
      "\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-93-1dcb841d4137>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Training for %d epochs...\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mn_epochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_epochs\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m     \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrnn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0mload_random_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_text\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mchunk_len\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrnn_optimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m     \u001b[0mloss_avg\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-81-5073d35df966>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(rnn, input, target, optimizer, criterion)\u001b[0m\n\u001b[1;32m     10\u001b[0m         \u001b[0mloss\u001b[0m\u001b[0;34m+=\u001b[0m\u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0mloss\u001b[0m \u001b[0;34m/=\u001b[0m \u001b[0mlength\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m     \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m     \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[1;32m     91\u001b[0m                 \u001b[0mproducts\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0mDefaults\u001b[0m \u001b[0mto\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m         \"\"\"\n\u001b[0;32m---> 93\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     94\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[1;32m     88\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m     89\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 90\u001b[0;31m         allow_unreachable=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m     91\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "### LSTM\n",
    "\n",
    "rnn = RNN(n_characters, hidden_size, n_characters, model_type=\"lstm\", n_layers=n_layers).to(device)\n",
    "rnn_optimizer = torch.optim.Adam(rnn.parameters(), lr=learning_rate)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "start = time.time()\n",
    "all_losses = []\n",
    "test_losses = []\n",
    "loss_avg = 0\n",
    "test_loss_avg = 0\n",
    "\n",
    "\n",
    "print(\"Training for %d epochs...\" % n_epochs)\n",
    "for epoch in range(1, n_epochs + 1):\n",
    "    loss = train(rnn, *load_random_batch(train_text, chunk_len, batch_size), rnn_optimizer, criterion)\n",
    "    loss_avg += loss\n",
    "    \n",
    "    test_loss = eval_test(rnn, *load_random_batch(test_text, chunk_len, batch_size))\n",
    "    test_loss_avg += test_loss\n",
    "\n",
    "    if epoch % print_every == 0:\n",
    "        print('[%s (%d %d%%) train loss: %.4f, test_loss: %.4f]' % (time_since(start), epoch, epoch / n_epochs * 100, loss, test_loss))\n",
    "        print(generate(rnn, 'Wh', 100, device=device), '\\n')\n",
    "\n",
    "    if epoch % plot_every == 0:\n",
    "        all_losses.append(loss_avg / plot_every)\n",
    "        test_losses.append(test_loss_avg / plot_every)\n",
    "        loss_avg = 0\n",
    "        test_loss_avg = 0\n",
    "        \n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as ticker\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(all_losses)\n",
    "plt.plot(test_losses, color='r')\n",
    "\n",
    "print(evaluate(rnn, prime_str='Th', predict_len=1000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### RNN + extra layer\n",
    "n_layers=3\n",
    "\n",
    "rnn = RNN(n_characters, hidden_size, n_characters, model_type=\"rnn\", n_layers=n_layers).to(device)\n",
    "rnn_optimizer = torch.optim.Adam(rnn.parameters(), lr=learning_rate)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "start = time.time()\n",
    "all_losses = []\n",
    "test_losses = []\n",
    "loss_avg = 0\n",
    "test_loss_avg = 0\n",
    "\n",
    "\n",
    "print(\"Training for %d epochs...\" % n_epochs)\n",
    "for epoch in range(1, n_epochs + 1):\n",
    "    loss = train(rnn, *load_random_batch(train_text, chunk_len, batch_size), rnn_optimizer, criterion)\n",
    "    loss_avg += loss\n",
    "    \n",
    "    test_loss = eval_test(rnn, *load_random_batch(test_text, chunk_len, batch_size))\n",
    "    test_loss_avg += test_loss\n",
    "\n",
    "    if epoch % print_every == 0:\n",
    "        print('[%s (%d %d%%) train loss: %.4f, test_loss: %.4f]' % (time_since(start), epoch, epoch / n_epochs * 100, loss, test_loss))\n",
    "        print(generate(rnn, 'Wh', 100, device=device), '\\n')\n",
    "\n",
    "    if epoch % plot_every == 0:\n",
    "        all_losses.append(loss_avg / plot_every)\n",
    "        test_losses.append(test_loss_avg / plot_every)\n",
    "        loss_avg = 0\n",
    "        test_loss_avg = 0\n",
    "        \n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as ticker\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(all_losses)\n",
    "plt.plot(test_losses, color='r')\n",
    "\n",
    "print(evaluate(rnn, prime_str='Th', predict_len=1000))"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
